import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
import matplotlib.pyplot as plt
import numpy as np
import os
import time
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# Configuration
dataset_dir = r'C:\Users\rafai\Desktop\Programs\Python\Ptyxiaki\πτυχιακή\Rafail_dataset\ntu\split_dataset'
model_save_dir = r'C:\Users\rafai\Desktop\Programs\Python\Ptyxiaki\πτυχιακή\Rafail_dataset\ntu\models'
results_dir = r'C:\Users\rafai\Desktop\Programs\Python\Ptyxiaki\πτυχιακή\Rafail_dataset\ntu\results'

# Create directories
os.makedirs(model_save_dir, exist_ok=True)
os.makedirs(results_dir, exist_ok=True)

# Hyperparameters
BATCH_SIZE = 32
LEARNING_RATE = 0.001
NUM_EPOCHS = 50
IMG_SIZE = (224, 224)
INPUT_SHAPE = (224, 224, 3)

# Check for GPU
print("GPU Available: ", tf.config.list_physical_devices('GPU'))
if tf.config.list_physical_devices('GPU'):
    print("Using GPU")
else:
    print("Using CPU")

# Data generators
def create_data_generators():
    # Training data generator with augmentation
    train_datagen = ImageDataGenerator(
        rescale=1.0/255.0,
        horizontal_flip=True,
        rotation_range=10,
        width_shift_range=0.1,
        height_shift_range=0.1,
        brightness_range=[0.8, 1.2],
        zoom_range=0.1,
        validation_split=0.0  # We have separate validation folder
    )
    
    # Validation and test data generators (no augmentation)
    val_test_datagen = ImageDataGenerator(rescale=1.0/255.0)
    
    generators = {}
    
    # Create generators for each split
    for split in ['train', 'val', 'test']:
        split_dir = os.path.join(dataset_dir, split)
        
        if os.path.exists(split_dir):
            if split == 'train':
                generator = train_datagen.flow_from_directory(
                    split_dir,
                    target_size=IMG_SIZE,
                    batch_size=BATCH_SIZE,
                    class_mode='categorical',
                    shuffle=True
                )
            else:
                generator = val_test_datagen.flow_from_directory(
                    split_dir,
                    target_size=IMG_SIZE,
                    batch_size=BATCH_SIZE,
                    class_mode='categorical',
                    shuffle=False
                )
            
            generators[split] = generator
            print(f"{split.capitalize()} generator: {generator.samples} samples")
        else:
            print(f"Warning: {split_dir} not found")
    
    return generators

# Create model
def create_model(num_classes):
    # Load pretrained ResNet50
    base_model = ResNet50(
        weights='imagenet',
        include_top=False,
        input_shape=INPUT_SHAPE
    )
    
    # Freeze early layers (optional - you can set to False for full fine-tuning)
    base_model.trainable = False
    
    # Unfreeze the last few layers for better performance
    for layer in base_model.layers[-20:]:  # Unfreeze last 20 layers
        layer.trainable = True
    
    # Add custom top layers
    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dropout(0.2),
        layers.Dense(num_classes, activation='softmax')
    ])
    
    return model

# Plot training history
def plot_training_history(history):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    # Plot loss
    ax1.plot(history.history['loss'], label='Train Loss')
    ax1.plot(history.history['val_loss'], label='Val Loss')
    ax1.set_title('Model Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    
    # Plot accuracy
    ax2.plot(history.history['accuracy'], label='Train Accuracy')
    ax2.plot(history.history['val_accuracy'], label='Val Accuracy')
    ax2.set_title('Model Accuracy')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.legend()
    
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, 'training_history.png'))
    plt.show()

# Test the model
def test_model(model, test_generator, class_names):
    print("Testing model...")
    
    # Reset generator
    test_generator.reset()
    
    # Make predictions
    predictions = model.predict(test_generator, steps=len(test_generator))
    y_pred = np.argmax(predictions, axis=1)
    
    # Get true labels
    y_true = test_generator.classes
    
    # Calculate accuracy
    accuracy = np.mean(y_pred == y_true)
    print(f'Test Accuracy: {accuracy:.4f}')
    
    # Classification report
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=class_names))
    
    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, 'confusion_matrix.png'))
    plt.show()

# Main training loop
def main():
    print("Loading datasets...")
    generators = create_data_generators()
    
    if 'train' not in generators:
        print("Error: Training dataset not found!")
        return
    
    # Get number of classes and class names
    num_classes = generators['train'].num_classes
    class_names = list(generators['train'].class_indices.keys())
    print(f"Number of classes: {num_classes}")
    print(f"Classes: {class_names}")
    
    # Create model
    print("Creating model...")
    model = create_model(num_classes)
    
    # Compile model
    model.compile(
        optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # Print model summary
    model.summary()
    
    # Callbacks
    callbacks = [
        ModelCheckpoint(
            os.path.join(model_save_dir, 'best_model.h5'),
            monitor='val_accuracy',
            save_best_only=True,
            save_weights_only=False,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.1,
            patience=7,
            min_lr=1e-7,
            verbose=1
        ),
        EarlyStopping(
            monitor='val_accuracy',
            patience=15,
            restore_best_weights=True,
            verbose=1
        )
    ]
    
    print("Starting training...")
    start_time = time.time()
    
    # Train the model
    history = model.fit(
        generators['train'],
        steps_per_epoch=len(generators['train']),
        epochs=NUM_EPOCHS,
        validation_data=generators['val'] if 'val' in generators else None,
        validation_steps=len(generators['val']) if 'val' in generators else None,
        callbacks=callbacks,
        verbose=1
    )
    
    training_time = time.time() - start_time
    print(f'Training complete in {training_time // 60:.0f}m {training_time % 60:.0f}s')
    
    # Save final model
    model.save(os.path.join(model_save_dir, 'final_model.h5'))
    
    # Plot training history
    plot_training_history(history)
    
    # Test the model
    if 'test' in generators:
        test_model(model, generators['test'], class_names)
    
    print("Training completed!")
    print(f"Models saved to: {model_save_dir}")
    print(f"Results saved to: {results_dir}")

if __name__ == "__main__":
    main()